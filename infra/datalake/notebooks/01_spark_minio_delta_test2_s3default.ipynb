{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29655f00-ec08-45fe-98e4-1632eb26709e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "032aa954-acf6-4c25-974c-5e72b9d138ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      ":: loading settings :: url = jar:file:/opt/spark/jars/ivy-2.5.3.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "Ivy Default Cache set to: /opt/spark/ivy/cache\n",
      "The jars for the packages stored in: /opt/spark/ivy/jars\n",
      "org.apache.spark#spark-hadoop-cloud_2.13 added as a dependency\n",
      "io.delta#delta-spark_2.13 added as a dependency\n",
      "org.apache.iceberg#iceberg-spark-runtime-4.0_2.13 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-27031d66-0b79-49aa-8bee-6b0ddfd11b10;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-hadoop-cloud_2.13;4.0.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.4.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.4.1 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.10.7 in central\n",
      "\tfound org.slf4j#slf4j-api;2.0.16 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-aws;3.4.1 in central\n",
      "\tfound org.wildfly.openssl#wildfly-openssl;2.2.5.Final in central\n",
      "\tfound software.amazon.awssdk#bundle;2.25.53 in central\n",
      "\tfound com.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.26 in central\n",
      "\tfound joda-time#joda-time;2.13.0 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-databind;2.18.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-annotations;2.18.2 in central\n",
      "\tfound com.fasterxml.jackson.core#jackson-core;2.18.2 in central\n",
      "\tfound com.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.18.2 in central\n",
      "\tfound org.apache.httpcomponents#httpclient;4.5.14 in central\n",
      "\tfound org.apache.httpcomponents#httpcore;4.4.16 in central\n",
      "\tfound commons-codec#commons-codec;1.17.2 in central\n",
      "\tfound org.apache.hadoop#hadoop-azure;3.4.1 in central\n",
      "\tfound com.microsoft.azure#azure-storage;7.0.1 in central\n",
      "\tfound com.microsoft.azure#azure-keyvault-core;1.0.0 in central\n",
      "\tfound org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.3.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-cloud-storage;3.4.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-annotations;3.4.1 in central\n",
      "\tfound org.apache.hadoop#hadoop-aliyun;3.4.1 in central\n",
      "\tfound com.aliyun.oss#aliyun-sdk-oss;3.13.2 in central\n",
      "\tfound org.jdom#jdom2;2.0.6.1 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-core;4.5.10 in central\n",
      "\tfound org.ini4j#ini4j;0.5.4 in central\n",
      "\tfound io.opentracing#opentracing-api;0.33.0 in central\n",
      "\tfound io.opentracing#opentracing-util;0.33.0 in central\n",
      "\tfound io.opentracing#opentracing-noop;0.33.0 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-ram;3.1.0 in central\n",
      "\tfound com.aliyun#aliyun-java-sdk-kms;2.11.0 in central\n",
      "\tfound org.codehaus.jettison#jettison;1.5.4 in central\n",
      "\tfound org.apache.hadoop#hadoop-azure-datalake;3.4.1 in central\n",
      "\tfound com.microsoft.azure#azure-data-lake-store-sdk;2.3.9 in central\n",
      "\tfound org.apache.hadoop#hadoop-huaweicloud;3.4.1 in central\n",
      "\tfound com.huaweicloud#esdk-obs-java;3.20.4.2 in central\n",
      "\tfound com.jamesmurty.utils#java-xmlbuilder;1.2 in central\n",
      "\tfound org.eclipse.jetty#jetty-util;11.0.24 in central\n",
      "\tfound org.eclipse.jetty#jetty-util-ajax;11.0.24 in central\n",
      "\tfound com.squareup.okhttp3#okhttp;3.12.12 in central\n",
      "\tfound com.squareup.okio#okio;1.17.6 in central\n",
      "\tfound io.delta#delta-spark_2.13;4.0.1 in central\n",
      "\tfound io.delta#delta-storage;4.0.1 in central\n",
      "\tfound org.antlr#antlr4-runtime;4.13.1 in central\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-4.0_2.13;1.10.1 in central\n",
      "downloading https://repo1.maven.org/maven2/org/apache/spark/spark-hadoop-cloud_2.13/4.0.1/spark-hadoop-cloud_2.13-4.0.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.spark#spark-hadoop-cloud_2.13;4.0.1!spark-hadoop-cloud_2.13.jar (98ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-spark_2.13/4.0.1/delta-spark_2.13-4.0.1.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-spark_2.13;4.0.1!delta-spark_2.13.jar (496ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/iceberg/iceberg-spark-runtime-4.0_2.13/1.10.1/iceberg-spark-runtime-4.0_2.13-1.10.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.iceberg#iceberg-spark-runtime-4.0_2.13;1.10.1!iceberg-spark-runtime-4.0_2.13.jar (2580ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-runtime/3.4.1/hadoop-client-runtime-3.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-runtime;3.4.1!hadoop-client-runtime.jar (2185ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.4.1/hadoop-aws-3.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aws;3.4.1!hadoop-aws.jar (113ms)\n",
      "downloading https://repo1.maven.org/maven2/org/wildfly/openssl/wildfly-openssl/2.2.5.Final/wildfly-openssl-2.2.5.Final.jar ...\n",
      "\t[SUCCESSFUL ] org.wildfly.openssl#wildfly-openssl;2.2.5.Final!wildfly-openssl.jar (119ms)\n",
      "downloading https://repo1.maven.org/maven2/software/amazon/awssdk/bundle/2.25.53/bundle-2.25.53.jar ...\n",
      "\t[SUCCESSFUL ] software.amazon.awssdk#bundle;2.25.53!bundle.jar (43750ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/cloud/bigdataoss/gcs-connector/hadoop3-2.2.26/gcs-connector-hadoop3-2.2.26-shaded.jar ...\n",
      "\t[SUCCESSFUL ] com.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.26!gcs-connector.jar (4104ms)\n",
      "downloading https://repo1.maven.org/maven2/joda-time/joda-time/2.13.0/joda-time-2.13.0.jar ...\n",
      "\t[SUCCESSFUL ] joda-time#joda-time;2.13.0!joda-time.jar (109ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-databind/2.18.2/jackson-databind-2.18.2.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-databind;2.18.2!jackson-databind.jar (190ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-annotations/2.18.2/jackson-annotations-2.18.2.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-annotations;2.18.2!jackson-annotations.jar (62ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/dataformat/jackson-dataformat-cbor/2.18.2/jackson-dataformat-cbor-2.18.2.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.18.2!jackson-dataformat-cbor.jar (67ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpclient/4.5.14/httpclient-4.5.14.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.httpcomponents#httpclient;4.5.14!httpclient.jar (137ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/httpcomponents/httpcore/4.4.16/httpcore-4.4.16.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.httpcomponents#httpcore;4.4.16!httpcore.jar (99ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure/3.4.1/hadoop-azure-3.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-azure;3.4.1!hadoop-azure.jar (1371ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-cloud-storage/3.4.1/hadoop-cloud-storage-3.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-cloud-storage;3.4.1!hadoop-cloud-storage.jar (45ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util/11.0.24/jetty-util-11.0.24.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-util;11.0.24!jetty-util.jar (77ms)\n",
      "downloading https://repo1.maven.org/maven2/org/eclipse/jetty/jetty-util-ajax/11.0.24/jetty-util-ajax-11.0.24.jar ...\n",
      "\t[SUCCESSFUL ] org.eclipse.jetty#jetty-util-ajax;11.0.24!jetty-util-ajax.jar (48ms)\n",
      "downloading https://repo1.maven.org/maven2/com/squareup/okhttp3/okhttp/3.12.12/okhttp-3.12.12.jar ...\n",
      "\t[SUCCESSFUL ] com.squareup.okhttp3#okhttp;3.12.12!okhttp.jar (81ms)\n",
      "downloading https://repo1.maven.org/maven2/com/squareup/okio/okio/1.17.6/okio-1.17.6.jar ...\n",
      "\t[SUCCESSFUL ] com.squareup.okio#okio;1.17.6!okio.jar (84ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-client-api/3.4.1/hadoop-client-api-3.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-client-api;3.4.1!hadoop-client-api.jar (1372ms)\n",
      "downloading https://repo1.maven.org/maven2/org/xerial/snappy/snappy-java/1.1.10.7/snappy-java-1.1.10.7.jar ...\n",
      "\t[SUCCESSFUL ] org.xerial.snappy#snappy-java;1.1.10.7!snappy-java.jar(bundle) (250ms)\n",
      "downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-api/2.0.16/slf4j-api-2.0.16.jar ...\n",
      "\t[SUCCESSFUL ] org.slf4j#slf4j-api;2.0.16!slf4j-api.jar (61ms)\n",
      "downloading https://repo1.maven.org/maven2/com/google/code/findbugs/jsr305/3.0.0/jsr305-3.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.google.code.findbugs#jsr305;3.0.0!jsr305.jar (80ms)\n",
      "downloading https://repo1.maven.org/maven2/com/fasterxml/jackson/core/jackson-core/2.18.2/jackson-core-2.18.2.jar ...\n",
      "\t[SUCCESSFUL ] com.fasterxml.jackson.core#jackson-core;2.18.2!jackson-core.jar (93ms)\n",
      "downloading https://repo1.maven.org/maven2/commons-codec/commons-codec/1.17.2/commons-codec-1.17.2.jar ...\n",
      "\t[SUCCESSFUL ] commons-codec#commons-codec;1.17.2!commons-codec.jar (70ms)\n",
      "downloading https://repo1.maven.org/maven2/com/microsoft/azure/azure-storage/7.0.1/azure-storage-7.0.1.jar ...\n",
      "\t[SUCCESSFUL ] com.microsoft.azure#azure-storage;7.0.1!azure-storage.jar (89ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/thirdparty/hadoop-shaded-guava/1.3.0/hadoop-shaded-guava-1.3.0.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop.thirdparty#hadoop-shaded-guava;1.3.0!hadoop-shaded-guava.jar (203ms)\n",
      "downloading https://repo1.maven.org/maven2/com/microsoft/azure/azure-keyvault-core/1.0.0/azure-keyvault-core-1.0.0.jar ...\n",
      "\t[SUCCESSFUL ] com.microsoft.azure#azure-keyvault-core;1.0.0!azure-keyvault-core.jar (49ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-annotations/3.4.1/hadoop-annotations-3.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-annotations;3.4.1!hadoop-annotations.jar (57ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aliyun/3.4.1/hadoop-aliyun-3.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-aliyun;3.4.1!hadoop-aliyun.jar (62ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-azure-datalake/3.4.1/hadoop-azure-datalake-3.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-azure-datalake;3.4.1!hadoop-azure-datalake.jar (51ms)\n",
      "downloading https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-huaweicloud/3.4.1/hadoop-huaweicloud-3.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.apache.hadoop#hadoop-huaweicloud;3.4.1!hadoop-huaweicloud.jar (52ms)\n",
      "downloading https://repo1.maven.org/maven2/com/aliyun/oss/aliyun-sdk-oss/3.13.2/aliyun-sdk-oss-3.13.2.jar ...\n",
      "\t[SUCCESSFUL ] com.aliyun.oss#aliyun-sdk-oss;3.13.2!aliyun-sdk-oss.jar (94ms)\n",
      "downloading https://repo1.maven.org/maven2/org/codehaus/jettison/jettison/1.5.4/jettison-1.5.4.jar ...\n",
      "\t[SUCCESSFUL ] org.codehaus.jettison#jettison;1.5.4!jettison.jar(bundle) (59ms)\n",
      "downloading https://repo1.maven.org/maven2/org/jdom/jdom2/2.0.6.1/jdom2-2.0.6.1.jar ...\n",
      "\t[SUCCESSFUL ] org.jdom#jdom2;2.0.6.1!jdom2.jar (74ms)\n",
      "downloading https://repo1.maven.org/maven2/com/aliyun/aliyun-java-sdk-core/4.5.10/aliyun-java-sdk-core-4.5.10.jar ...\n",
      "\t[SUCCESSFUL ] com.aliyun#aliyun-java-sdk-core;4.5.10!aliyun-java-sdk-core.jar (65ms)\n",
      "downloading https://repo1.maven.org/maven2/com/aliyun/aliyun-java-sdk-ram/3.1.0/aliyun-java-sdk-ram-3.1.0.jar ...\n",
      "\t[SUCCESSFUL ] com.aliyun#aliyun-java-sdk-ram;3.1.0!aliyun-java-sdk-ram.jar (371ms)\n",
      "downloading https://repo1.maven.org/maven2/com/aliyun/aliyun-java-sdk-kms/2.11.0/aliyun-java-sdk-kms-2.11.0.jar ...\n",
      "\t[SUCCESSFUL ] com.aliyun#aliyun-java-sdk-kms;2.11.0!aliyun-java-sdk-kms.jar (70ms)\n",
      "downloading https://repo1.maven.org/maven2/org/ini4j/ini4j/0.5.4/ini4j-0.5.4.jar ...\n",
      "\t[SUCCESSFUL ] org.ini4j#ini4j;0.5.4!ini4j.jar (63ms)\n",
      "downloading https://repo1.maven.org/maven2/io/opentracing/opentracing-api/0.33.0/opentracing-api-0.33.0.jar ...\n",
      "\t[SUCCESSFUL ] io.opentracing#opentracing-api;0.33.0!opentracing-api.jar (54ms)\n",
      "downloading https://repo1.maven.org/maven2/io/opentracing/opentracing-util/0.33.0/opentracing-util-0.33.0.jar ...\n",
      "\t[SUCCESSFUL ] io.opentracing#opentracing-util;0.33.0!opentracing-util.jar (41ms)\n",
      "downloading https://repo1.maven.org/maven2/io/opentracing/opentracing-noop/0.33.0/opentracing-noop-0.33.0.jar ...\n",
      "\t[SUCCESSFUL ] io.opentracing#opentracing-noop;0.33.0!opentracing-noop.jar (43ms)\n",
      "downloading https://repo1.maven.org/maven2/com/microsoft/azure/azure-data-lake-store-sdk/2.3.9/azure-data-lake-store-sdk-2.3.9.jar ...\n",
      "\t[SUCCESSFUL ] com.microsoft.azure#azure-data-lake-store-sdk;2.3.9!azure-data-lake-store-sdk.jar (55ms)\n",
      "downloading https://repo1.maven.org/maven2/com/huaweicloud/esdk-obs-java/3.20.4.2/esdk-obs-java-3.20.4.2.jar ...\n",
      "\t[SUCCESSFUL ] com.huaweicloud#esdk-obs-java;3.20.4.2!esdk-obs-java.jar (92ms)\n",
      "downloading https://repo1.maven.org/maven2/com/jamesmurty/utils/java-xmlbuilder/1.2/java-xmlbuilder-1.2.jar ...\n",
      "\t[SUCCESSFUL ] com.jamesmurty.utils#java-xmlbuilder;1.2!java-xmlbuilder.jar (53ms)\n",
      "downloading https://repo1.maven.org/maven2/io/delta/delta-storage/4.0.1/delta-storage-4.0.1.jar ...\n",
      "\t[SUCCESSFUL ] io.delta#delta-storage;4.0.1!delta-storage.jar (63ms)\n",
      "downloading https://repo1.maven.org/maven2/org/antlr/antlr4-runtime/4.13.1/antlr4-runtime-4.13.1.jar ...\n",
      "\t[SUCCESSFUL ] org.antlr#antlr4-runtime;4.13.1!antlr4-runtime.jar (90ms)\n",
      ":: resolution report :: resolve 39562ms :: artifacts dl 64910ms\n",
      "\t:: modules in use:\n",
      "\tcom.aliyun#aliyun-java-sdk-core;4.5.10 from central in [default]\n",
      "\tcom.aliyun#aliyun-java-sdk-kms;2.11.0 from central in [default]\n",
      "\tcom.aliyun#aliyun-java-sdk-ram;3.1.0 from central in [default]\n",
      "\tcom.aliyun.oss#aliyun-sdk-oss;3.13.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-annotations;2.18.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-core;2.18.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.core#jackson-databind;2.18.2 from central in [default]\n",
      "\tcom.fasterxml.jackson.dataformat#jackson-dataformat-cbor;2.18.2 from central in [default]\n",
      "\tcom.google.cloud.bigdataoss#gcs-connector;hadoop3-2.2.26 from central in [default]\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcom.huaweicloud#esdk-obs-java;3.20.4.2 from central in [default]\n",
      "\tcom.jamesmurty.utils#java-xmlbuilder;1.2 from central in [default]\n",
      "\tcom.microsoft.azure#azure-data-lake-store-sdk;2.3.9 from central in [default]\n",
      "\tcom.microsoft.azure#azure-keyvault-core;1.0.0 from central in [default]\n",
      "\tcom.microsoft.azure#azure-storage;7.0.1 from central in [default]\n",
      "\tcom.squareup.okhttp3#okhttp;3.12.12 from central in [default]\n",
      "\tcom.squareup.okio#okio;1.17.6 from central in [default]\n",
      "\tcommons-codec#commons-codec;1.17.2 from central in [default]\n",
      "\tio.delta#delta-spark_2.13;4.0.1 from central in [default]\n",
      "\tio.delta#delta-storage;4.0.1 from central in [default]\n",
      "\tio.opentracing#opentracing-api;0.33.0 from central in [default]\n",
      "\tio.opentracing#opentracing-noop;0.33.0 from central in [default]\n",
      "\tio.opentracing#opentracing-util;0.33.0 from central in [default]\n",
      "\tjoda-time#joda-time;2.13.0 from central in [default]\n",
      "\torg.antlr#antlr4-runtime;4.13.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aliyun;3.4.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-annotations;3.4.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-aws;3.4.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure;3.4.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-azure-datalake;3.4.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.4.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.4.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-cloud-storage;3.4.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-huaweicloud;3.4.1 from central in [default]\n",
      "\torg.apache.hadoop.thirdparty#hadoop-shaded-guava;1.3.0 from central in [default]\n",
      "\torg.apache.httpcomponents#httpclient;4.5.14 from central in [default]\n",
      "\torg.apache.httpcomponents#httpcore;4.4.16 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-4.0_2.13;1.10.1 from central in [default]\n",
      "\torg.apache.spark#spark-hadoop-cloud_2.13;4.0.1 from central in [default]\n",
      "\torg.codehaus.jettison#jettison;1.5.4 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util;11.0.24 from central in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;11.0.24 from central in [default]\n",
      "\torg.ini4j#ini4j;0.5.4 from central in [default]\n",
      "\torg.jdom#jdom2;2.0.6.1 from central in [default]\n",
      "\torg.slf4j#slf4j-api;2.0.16 from central in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;2.2.5.Final from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.10.7 from central in [default]\n",
      "\tsoftware.amazon.awssdk#bundle;2.25.53 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\tsoftware.amazon.awssdk#bundle;2.24.6 by [software.amazon.awssdk#bundle;2.25.53] in [default]\n",
      "\torg.eclipse.jetty#jetty-util-ajax;9.4.53.v20231009 by [org.eclipse.jetty#jetty-util-ajax;11.0.24] in [default]\n",
      "\torg.wildfly.openssl#wildfly-openssl;1.1.3.Final by [org.wildfly.openssl#wildfly-openssl;2.2.5.Final] in [default]\n",
      "\tcom.squareup.okhttp3#okhttp;3.14.2 by [com.squareup.okhttp3#okhttp;3.12.12] in [default]\n",
      "\tcom.squareup.okio#okio;1.17.2 by [com.squareup.okio#okio;1.17.6] in [default]\n",
      "\tcom.squareup.okio#okio;1.15.0 by [com.squareup.okio#okio;1.17.6] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   54  |   48  |   48  |   6   ||   48  |   48  |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-27031d66-0b79-49aa-8bee-6b0ddfd11b10\n",
      "\tconfs: [default]\n",
      "\t48 artifacts copied, 0 already retrieved (714309kB/16010ms)\n",
      "26/01/11 23:35:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .master(\"spark://spark-master:7077\")\n",
    "         .appName(\"read-minio-defaults\")\n",
    "         .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00c8c6b9-44b9-4984-a09a-ab4725b84c58",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 23:36:47 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "SLF4J: Failed to load class \"org.slf4j.impl.StaticLoggerBinder\".\n",
      "SLF4J: Defaulting to no-operation (NOP) logger implementation\n",
      "SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.\n",
      "26/01/11 23:36:58 WARN FileStreamSink: Assume no metadata directory. Error while looking for metadata directory in the path: s3a://bronze/flights/.\n",
      "java.io.FileNotFoundException: No such file or directory: s3a://bronze/flights\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:4156)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:4007)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getFileStatus$22(S3AFileSystem.java:3984)\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.invokeTrackingDuration(IOStatisticsBinding.java:547)\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:528)\n",
      "\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:449)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2865)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2884)\n",
      "\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getFileStatus(S3AFileSystem.java:3982)\n",
      "\tat org.apache.spark.sql.execution.streaming.FileStreamSink$.hasMetadata(FileStreamSink.scala:56)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:381)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.org$apache$spark$sql$catalyst$analysis$ResolveDataSource$$loadV1BatchSource(ResolveDataSource.scala:143)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.$anonfun$applyOrElse$2(ResolveDataSource.scala:61)\n",
      "\tat scala.Option.getOrElse(Option.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:61)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource$$anonfun$apply$1.applyOrElse(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$3(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:86)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.$anonfun$resolveOperatorsUpWithPruning$1(AnalysisHelper.scala:139)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:416)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning(AnalysisHelper.scala:135)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUpWithPruning$(AnalysisHelper.scala:131)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUpWithPruning(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp(AnalysisHelper.scala:112)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.resolveOperatorsUp$(AnalysisHelper.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:37)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:45)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.ResolveDataSource.apply(ResolveDataSource.scala:43)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$2(RuleExecutor.scala:242)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft(LinearSeq.scala:183)\n",
      "\tat scala.collection.LinearSeqOps.foldLeft$(LinearSeq.scala:179)\n",
      "\tat scala.collection.immutable.List.foldLeft(List.scala:79)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1(RuleExecutor.scala:239)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$execute$1$adapted(RuleExecutor.scala:231)\n",
      "\tat scala.collection.immutable.List.foreach(List.scala:334)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:231)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:340)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$execute$1(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withNewAnalysisContext(Analyzer.scala:234)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:336)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:299)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.$anonfun$executeAndTrack$1(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:89)\n",
      "\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.executeAndTrack(RuleExecutor.scala:201)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:190)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:111)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:71)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:423)\n",
      "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:330)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:110)\n",
      "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:148)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:654)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:278)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:277)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:110)\n",
      "\tat scala.util.Try$.apply(Try.scala:217)\n",
      "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1378)\n",
      "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:46)\n",
      "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:58)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:121)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:80)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$1(Dataset.scala:115)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:804)\n",
      "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:113)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:109)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.load(DataFrameReader.scala:58)\n",
      "\tat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:392)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:259)\n",
      "\tat org.apache.spark.sql.classic.DataFrameReader.csv(DataFrameReader.scala:58)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:184)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:108)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: s3a://bronze/flights. SQLSTATE: 42K03",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43ms3a://bronze/flights/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m5\u001b[39m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/readwriter.py:838\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    837\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 838\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    840\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_remote_only():\n\u001b[1;32m    841\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrdd\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RDD  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py:288\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    284\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    290\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: s3a://bronze/flights. SQLSTATE: 42K03"
     ]
    }
   ],
   "source": [
    "df = spark.read.option(\"header\",\"true\").csv(\"s3a://bronze/flights/\")\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05afaa38-cfc0-42a9-a830-173d63690be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import current_date\n",
    "df_silver = df.withColumn(\"ingestion_date\", current_date())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b2044cc-a6a5-498d-ad1d-f32325b17e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "silver_out = \"s3a://silver/flights_delta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ed6c9d23-de40-4fbf-b5bf-86124db65af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_silver.write.format(\"delta\").mode(\"overwrite\").save(silver_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "065ab65c-0beb-47c9-b2fd-db14579880d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_delta = spark.read.format(\"delta\").load(silver_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "156e0040-5f5b-4235-ae8d-5b486abe9c9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26/01/11 13:34:04 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+--------+------+-----------+--------------+\n",
      "|    date|delay|distance|origin|destination|ingestion_date|\n",
      "+--------+-----+--------+------+-----------+--------------+\n",
      "|02151800|  108|     290|   ORD|        MSP|    2026-01-11|\n",
      "|02151800|  142|     772|   ORD|        DEN|    2026-01-11|\n",
      "|02151303|   16|    1516|   ORD|        LAX|    2026-01-11|\n",
      "|02151157|    7|    1316|   ORD|        LAS|    2026-01-11|\n",
      "|02151818|   55|    1511|   ORD|        PDX|    2026-01-11|\n",
      "|02151033|   12|     873|   ORD|        MCO|    2026-01-11|\n",
      "|02150941|    0|    1499|   ORD|        SNA|    2026-01-11|\n",
      "|02151320|   17|    1604|   ORD|        SFO|    2026-01-11|\n",
      "|02151804|    2|    1497|   ORD|        SAN|    2026-01-11|\n",
      "|02152000|   17|     119|   ORD|        GRR|    2026-01-11|\n",
      "|02151502|   15|     260|   ORD|        DSM|    2026-01-11|\n",
      "|02151935|   73|     879|   ORD|        TPA|    2026-01-11|\n",
      "|02151315|   -1|     879|   ORD|        TPA|    2026-01-11|\n",
      "|02150957|    0|     939|   ORD|        MTJ|    2026-01-11|\n",
      "|02152001|  123|     804|   ORD|        IAH|    2026-01-11|\n",
      "|02151945|   42|     558|   ORD|        RIC|    2026-01-11|\n",
      "|02151502|   89|     625|   ORD|        EWR|    2026-01-11|\n",
      "|02150653|   -5|     974|   ORD|        RSW|    2026-01-11|\n",
      "|02150947|   -1|    1316|   ORD|        LAS|    2026-01-11|\n",
      "|02152030|   24|    1604|   ORD|        SFO|    2026-01-11|\n",
      "+--------+-----+--------+------+-----------+--------------+\n",
      "only showing top 20 rows\n"
     ]
    }
   ],
   "source": [
    "df_delta.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b035644a-3e5f-4d33-9747-476d5e421bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eab4a76-c896-48ee-b719-f59a3a5f05d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark (Docker)",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
